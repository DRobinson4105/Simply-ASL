{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.mhformer import Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "N_KPTS = 133\n",
    "N_FRAMES = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.frames = 60\n",
    "        self.n_joints = 133\n",
    "        self.channel = 256\n",
    "        self.out_joints = 133\n",
    "        self.layers = 4\n",
    "        self.d_hid = 512\n",
    "\n",
    "args = Args()\n",
    "model = Model(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (norm_1): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm_2): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm_3): LayerNorm((60,), eps=1e-05, elementwise_affine=True)\n",
       "  (Transformer_encoder_1): Transformer(\n",
       "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.067)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.133)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (Transformer_encoder_2): Transformer(\n",
       "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.067)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.133)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (Transformer_encoder_3): Transformer(\n",
       "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.067)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.133)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=60, out_features=180, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "        (norm2): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=60, out_features=120, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (embedding_1): Conv1d(266, 256, kernel_size=(1,), stride=(1,))\n",
       "  (embedding_2): Conv1d(266, 256, kernel_size=(1,), stride=(1,))\n",
       "  (embedding_3): Conv1d(266, 256, kernel_size=(1,), stride=(1,))\n",
       "  (Transformer_hypothesis): Transformer(\n",
       "    (pos_drop_1): Dropout(p=0.1, inplace=False)\n",
       "    (pos_drop_2): Dropout(p=0.1, inplace=False)\n",
       "    (pos_drop_3): Dropout(p=0.1, inplace=False)\n",
       "    (SHR_blocks): ModuleList(\n",
       "      (0): SHR_Block(\n",
       "        (norm1_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm1_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm1_3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn_1): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_2): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_3): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SHR_Block(\n",
       "        (norm1_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm1_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm1_3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn_1): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_2): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_3): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.067)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SHR_Block(\n",
       "        (norm1_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm1_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm1_3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn_1): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_2): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_3): Attention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.133)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (CHI_blocks): ModuleList(\n",
       "      (0): CHI_Block(\n",
       "        (norm3_11): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_12): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_13): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_21): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_22): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_23): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_31): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_32): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm3_33): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn_1): Cross_Attention(\n",
       "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_2): Cross_Attention(\n",
       "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attn_3): Cross_Attention(\n",
       "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_path): DropPath(drop_prob=0.200)\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (regression): Sequential(\n",
       "    (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Conv1d(768, 399, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model/mhformer.pth\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"IMG_8212.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data = data[\"instance_info\"][0][\"keypoints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 133, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_2d = torch.tensor(data, dtype=torch.float, device=device)\n",
    "keypoints_2d = keypoints_2d.unsqueeze(0)\n",
    "keypoints_2d = keypoints_2d.repeat((60, 1, 1))\n",
    "keypoints_2d = keypoints_2d.unsqueeze(0)\n",
    "keypoints_3d = model(keypoints_2d)\n",
    "keypoints_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = keypoints_3d[0, 0].detach().cpu().numpy()\n",
    "np.savez(\"keypoints\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2493.8547, 1714.1366],\n",
       "          [2609.0972, 1611.3046],\n",
       "          [2482.2881, 1633.0760],\n",
       "          ...,\n",
       "          [2091.5415, 3358.7749],\n",
       "          [2090.1392, 3358.8303],\n",
       "          [2092.8386, 3358.3726]],\n",
       "\n",
       "         [[2493.8547, 1714.1366],\n",
       "          [2609.0972, 1611.3046],\n",
       "          [2482.2881, 1633.0760],\n",
       "          ...,\n",
       "          [2091.5415, 3358.7749],\n",
       "          [2090.1392, 3358.8303],\n",
       "          [2092.8386, 3358.3726]],\n",
       "\n",
       "         [[2493.8547, 1714.1366],\n",
       "          [2609.0972, 1611.3046],\n",
       "          [2482.2881, 1633.0760],\n",
       "          ...,\n",
       "          [2091.5415, 3358.7749],\n",
       "          [2090.1392, 3358.8303],\n",
       "          [2092.8386, 3358.3726]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2493.8547, 1714.1366],\n",
       "          [2609.0972, 1611.3046],\n",
       "          [2482.2881, 1633.0760],\n",
       "          ...,\n",
       "          [2091.5415, 3358.7749],\n",
       "          [2090.1392, 3358.8303],\n",
       "          [2092.8386, 3358.3726]],\n",
       "\n",
       "         [[2493.8547, 1714.1366],\n",
       "          [2609.0972, 1611.3046],\n",
       "          [2482.2881, 1633.0760],\n",
       "          ...,\n",
       "          [2091.5415, 3358.7749],\n",
       "          [2090.1392, 3358.8303],\n",
       "          [2092.8386, 3358.3726]],\n",
       "\n",
       "         [[2493.8547, 1714.1366],\n",
       "          [2609.0972, 1611.3046],\n",
       "          [2482.2881, 1633.0760],\n",
       "          ...,\n",
       "          [2091.5415, 3358.7749],\n",
       "          [2090.1392, 3358.8303],\n",
       "          [2092.8386, 3358.3726]]]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.cat((keypoints_2d, torch.zeros((1, 60, 133, 1), device=device)), dim=-1)\n",
    "np.savez('idk', temp[0, 0].cpu().numpy(), keypoints_3d[0, 0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -32.4070, -810.9669, -304.9438],\n",
       "          [  13.6878, -808.4675, -335.5195],\n",
       "          [ -45.7688, -834.4612, -236.6948],\n",
       "          ...,\n",
       "          [-335.9191,  228.8342,   41.5663],\n",
       "          [-333.3473,  227.9351,   28.5436],\n",
       "          [-327.6690,  218.1649,   27.0325]],\n",
       "\n",
       "         [[ -44.4964, -790.9933, -336.1886],\n",
       "          [   1.4576, -788.0034, -367.2071],\n",
       "          [ -57.5554, -814.1517, -267.6473],\n",
       "          ...,\n",
       "          [-346.9630,  243.7602,   10.5157],\n",
       "          [-344.5704,  242.7551,   -2.5796],\n",
       "          [-339.1542,  232.9596,   -4.0230]],\n",
       "\n",
       "         [[ -26.5347, -817.0125, -304.4930],\n",
       "          [  19.4820, -814.5652, -335.2736],\n",
       "          [ -39.7470, -840.5464, -236.1372],\n",
       "          ...,\n",
       "          [-328.8535,  224.8457,   43.2019],\n",
       "          [-326.2545,  223.9070,   30.1350],\n",
       "          [-320.5480,  214.1002,   28.6071]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ -43.0672, -797.3382, -325.4843],\n",
       "          [   2.8726, -794.5076, -356.1923],\n",
       "          [ -56.3128, -820.5638, -257.2352],\n",
       "          ...,\n",
       "          [-345.8758,  239.4707,   20.5527],\n",
       "          [-343.3969,  238.4770,    7.4994],\n",
       "          [-337.8623,  228.6965,    6.0398]],\n",
       "\n",
       "         [[ -31.5102, -812.0137, -310.8791],\n",
       "          [  14.4005, -809.4663, -341.6469],\n",
       "          [ -44.6236, -835.4891, -242.6318],\n",
       "          ...,\n",
       "          [-333.9507,  228.7776,   36.2451],\n",
       "          [-331.3904,  227.8384,   23.2062],\n",
       "          [-325.7481,  218.0353,   21.6922]],\n",
       "\n",
       "         [[ -34.0116, -818.4712, -298.2120],\n",
       "          [  12.1064, -816.0552, -328.6828],\n",
       "          [ -47.3675, -842.0994, -230.0763],\n",
       "          ...,\n",
       "          [-337.2827,  224.5895,   48.0350],\n",
       "          [-334.6005,  223.7064,   35.0273],\n",
       "          [-328.8271,  213.9420,   33.5110]]]], device='cuda:0',\n",
       "       grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_3d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
